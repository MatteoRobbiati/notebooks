{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrxupRHaQcEN7Y64UPARiz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatteoRobbiati/notebooks/blob/main/QTI-QML-tutorial/QML_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QTI-TH Forum: a snapshot of Quantum Machine Learning\n",
        "\n",
        "In the first part of the notebook we see how to concretely implement the QML ingredients using `qibo`. We need:\n",
        "\n",
        "1. A parameteric model $\\mathcal{M}$;\n",
        "2. A way to embed input data $x$ into $\\mathcal{M}$;\n",
        "3. A predictor for estimating the output $y$;\n",
        "4. A loss function $\\mathcal{J}$;\n",
        "5. An optimizer $\\mathcal{O}$."
      ],
      "metadata": {
        "id": "8gpcQ1t3DVrp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nKU4ATqxZDw"
      },
      "outputs": [],
      "source": [
        "# install qibo\n",
        "!pip install qibo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import qibo's packages\n",
        "import qibo\n",
        "from qibo import gates, hamiltonians, derivative\n",
        "from qibo.models import Circuit\n",
        "\n",
        "# some useful python package\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_theme(style='whitegrid', font_scale=1.5)\n",
        "\n",
        "# to interact with the operating system\n",
        "import os\n",
        "\n",
        "# numpy backend is enough for a 1-qubit model\n",
        "qibo.set_backend('numpy')"
      ],
      "metadata": {
        "id": "VoUQqEHWxeSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Model $\\mathcal{M}$: a variational quantum circuit\n",
        "\n",
        "We are going to use a variational quantum circuit as Machine Learning model.\n",
        "In order to do this, we will use the circuit's parameters as variational parameters during the train. "
      ],
      "metadata": {
        "id": "hUd7L93Ux8Xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nqubits = 1\n",
        "layers = 2\n",
        "\n",
        "c = Circuit(nqubits)\n",
        "for q in range(nqubits):\n",
        "  # an Hadamard gate at the beginning\n",
        "  c.add(gates.H(q=q))\n",
        "  # and a sequence of rotation layers as model\n",
        "  for l in range(layers):\n",
        "    c.add(gates.RY(q=q, theta=0))\n",
        "    c.add(gates.RY(q=q, theta=0))\n",
        "    c.add(gates.RZ(q=q, theta=0))\n",
        "  c.add(gates.M(0))\n",
        "\n",
        "print(c.summary())"
      ],
      "metadata": {
        "id": "Y7XGIQTtxr0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Embedding of the input data $x$ into $\\mathcal{M}$\n",
        "\n",
        "This specific sequence of gates is important if we want to implement the following ansatz:\n",
        "\n",
        "<center><img src=\"https://github.com/MatteoRobbiati/notebooks/blob/main/QTI-QML-tutorial/images/re-uploading.png?raw=true\" alt=\"drawing\" width=\"800\"/></center>\n"
      ],
      "metadata": {
        "id": "eNqSY_wcFqMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inject_data(circuit, parameters, x):\n",
        "  \"\"\"Prototype function for the embedding of x.\"\"\"\n",
        "  params = []\n",
        "  index = 0\n",
        "  \n",
        "  for q in range(nqubits):\n",
        "    for l in range(layers):\n",
        "      # embed the first feature\n",
        "      params.append(parameters[index] * x)\n",
        "      params.append(parameters[index + 1])\n",
        "      params.append(parameters[index + 2])\n",
        "      index += 3\n",
        "\n",
        "  circuit.set_parameters(params)\n",
        "  return circuit"
      ],
      "metadata": {
        "id": "71BkNfnky-Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting new parameters\n",
        "nparams = len(c.get_parameters())\n",
        "initial_parameters = np.random.randn(nparams) * 5"
      ],
      "metadata": {
        "id": "ZhyOaWcTz4CY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Predictor:  $\\hat{y} = \\langle \\hat{O} \\rangle$\n",
        "\n",
        "Applying the circuit $C(x, \\theta)$ to an initial state $ |q_i\\rangle \\equiv | 0 \\rangle$, we get the final state of the qubit $|q_f\\rangle$. In this example, we decide to use as predictor of $y$ the expected value of a non-interacting Pauli Z observable."
      ],
      "metadata": {
        "id": "q1iFA98hMlna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define an hamiltonian\n",
        "# a Pauli-Z\n",
        "h = hamiltonians.Z(nqubits)\n",
        "\n",
        "# a dummy value for x\n",
        "x = 0.2\n",
        "\n",
        "# set them into the circuit together with an x\n",
        "c = inject_data(c, initial_parameters, x)\n",
        "\n",
        "# evaluating E[O]\n",
        "h.expectation(c.execute(nshots=1000).state())"
      ],
      "metadata": {
        "id": "czJAb0380odQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. MSE loss function: $\\mathcal{J}_{mse}$\n",
        "\n",
        "We will quantify the goodness of our model using:\n",
        "\n",
        "$$ \\mathcal{J}_{mse} = \\frac{1}{N_{data}} \\sum_{i=1}^{N_{data}}\\bigl(y_{meas,i} - y_{est,i}\\bigr)^2,$$\n",
        "\n",
        "Where $y_{est,i}$ is the expected value of $Z$ over the final 1-qubit state once $C(x_i, \\theta)$ has been applied:\n",
        "\n",
        "$$ y_{est,i} = \\langle q_f | Z  | q_f \\rangle  = \\langle 0 | C(x_i,\\theta)^{\\dagger} Z C(x_i,\\theta) | 0 \\rangle.  $$\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "87WNGygO3ycB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 5. A gradient descent optimization!\n",
        "\n",
        "We want to perform a gradient descent optimization. For doing this, we need to calculate the gradients of the loss function with respect to the variational parameters. We will use a formula, called **Parameter Shift Rule**, thanks to which we are able to calculate exactly these derivatives by executing the same circuit twice after a proper shift of the target parameter.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "dxmwIrbBpfie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Quick explanation\n",
        "Let me call $f(\\mu)$ the expected value of our observable $Z$ over the final state $|q_f \\rangle$ obtained by applying a circuit $C(\\theta)$ to an initial state: $|q_f\\rangle  \\equiv C(\\theta)|q_i\\rangle$ and such that $\\mu \\in \\theta$:\n",
        "\n",
        "$$ f(\\mu) = \\langle q_i | C(\\theta)^{\\dagger} Z C(\\theta)| q_i\\rangle. $$\n",
        "\n",
        "In a few words, if some conditions (you can find reference [here](https://arxiv.org/abs/1811.11184)) are satisfied, we can evaluate the derivative of $f(\\mu)$ as follows:\n",
        "\n",
        "$$ \\partial_{\\mu} f = r\\bigl[ f(\\mu^+) - f(\\mu^-) \\bigr],$$\n",
        "\n",
        "with $\\mu^{\\pm}$ are two specific shifted values of $\\mu$ (you can find all into the reference).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "EJqBF-yipkQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Last step: derivative of $J$\n",
        "\n",
        "Since we need the derivative of the loss function, we will follow this procedure:\n",
        "\n",
        "- injecting $x_i$ into the circuit obtaining $C(x_i, \\theta)$;\n",
        "- calculating prediction as $f$;\n",
        "- for each $\\mu \\in \\theta$ we calculate $f(\\mu^{\\pm})$;\n",
        "- use PSR for calculating each $\\partial_{\\mu}f$."
      ],
      "metadata": {
        "id": "4xBKCIhWcrJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's put it all together in a variational quantum regressor"
      ],
      "metadata": {
        "id": "kn4v1p1tNU2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class vqregressor:\n",
        "\n",
        "  def __init__(self, data, labels, layers, nqubits=1):\n",
        "    \"\"\"Class constructor.\"\"\"\n",
        "    # some general features of the QML model\n",
        "    self.nqubits = nqubits\n",
        "    self.layers = layers\n",
        "    self.data = data\n",
        "    self.labels = labels\n",
        "\n",
        "    # initialize the circuit and extract the number of parameters\n",
        "    self.circuit = self.ansatz(nqubits, layers)\n",
        "    print(self.circuit.draw())\n",
        "\n",
        "    # get the number of parameters\n",
        "    self.nparams = len(self.circuit.get_parameters())\n",
        "    # set the initial value of the variational parameters\n",
        "    self.params = np.random.randn(self.nparams)\n",
        "    # scaling factor for custom parameter shift rule\n",
        "    self.scale_factors = np.ones(self.nparams)\n",
        "\n",
        "    # define the observable\n",
        "    self.h = hamiltonians.Z(nqubits)\n",
        "\n",
        "# ---------------------------- ANSATZ ------------------------------------------\n",
        "\n",
        "  def ansatz(self, nqubits, layers):\n",
        "    \"\"\"Here we implement the variational model ansatz.\"\"\"\n",
        "    c = Circuit(nqubits)\n",
        "    for q in range(nqubits):\n",
        "      c.add(gates.H(q=q))\n",
        "      for l in range(layers):\n",
        "        c.add(gates.RY(q=q, theta=0))\n",
        "        c.add(gates.RY(q=q, theta=0))\n",
        "        c.add(gates.RZ(q=q, theta=0))\n",
        "    c.add(gates.M(0))\n",
        "\n",
        "    return c\n",
        "\n",
        "# --------------------------- RE-UPLOADING -------------------------------------\n",
        "\n",
        "  def inject_data(self, x):\n",
        "    \"\"\"Here we combine x and params in order to perform re-uploading.\"\"\"\n",
        "    params = []\n",
        "    index = 0\n",
        "    \n",
        "    for q in range(self.nqubits):\n",
        "      for l in range(self.layers):\n",
        "        # embed X\n",
        "        params.append(self.params[index] * x)\n",
        "        params.append(self.params[index + 1])\n",
        "        params.append(self.params[index + 2])\n",
        "        # update scale factors \n",
        "        # equal to x only when x is involved\n",
        "        self.scale_factors[index] = x\n",
        "        # we have three parameters per layer\n",
        "        index += 3\n",
        "\n",
        "    # update circuit's parameters\n",
        "    self.circuit.set_parameters(params)\n",
        "\n",
        "\n",
        "# ------------------------------- PREDICTIONS ----------------------------------\n",
        "\n",
        "  def one_prediction(self, x):\n",
        "    \"\"\"This function calculates one prediction with fixed x.\"\"\"\n",
        "    self.inject_data(x)\n",
        "\n",
        "    return self.h.expectation(self.circuit.execute().state())\n",
        "\n",
        "\n",
        "  def predict_sample(self):\n",
        "    \"\"\"This function returns all predictions.\"\"\"\n",
        "    predictions = []\n",
        "    for x in self.data:\n",
        "      predictions.append(self.one_prediction(x))\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "# ------------------------ PERFORMING GRADIENT DESCENT -------------------------\n",
        "\n",
        "\n",
        "  def circuit_derivative(self):\n",
        "    \"\"\"Derivatives of the expected value of the target observable with respect \n",
        "    to the variational parameters of the circuit are performed via parameter-shift\n",
        "    rule (PSR).\"\"\"\n",
        "    dcirc = np.zeros(self.nparams)   \n",
        "    \n",
        "    for par in range(self.nparams):\n",
        "      # read qibo documentation for more information about this PSR implementation\n",
        "      dcirc[par] = qibo.derivative.parameter_shift(\n",
        "          circuit = self.circuit, \n",
        "          hamiltonian = self.h, \n",
        "          parameter_index = par, \n",
        "          scale_factor = self.scale_factors[par]\n",
        "          )\n",
        "    \n",
        "    return dcirc\n",
        "\n",
        "\n",
        "  def evaluate_loss_gradients(self):\n",
        "    \"\"\"This function calculates the derivative of the loss function with respect\n",
        "    to the variational parameters of the model.\"\"\"\n",
        "\n",
        "    # we need the derivative of the loss\n",
        "    # nparams-long vector\n",
        "    dloss = np.zeros(self.nparams)\n",
        "    # we also keep track of the loss value\n",
        "    loss = 0\n",
        "\n",
        "    # cycle on all the sample\n",
        "    for x, y in zip(self.data, self.labels):\n",
        "      # calculate prediction\n",
        "      prediction = self.one_prediction(x)\n",
        "      # calculate loss \n",
        "      mse = (prediction - y)\n",
        "      loss += mse**2\n",
        "      # derivative of E[O] with respect all thetas\n",
        "      dcirc = self.circuit_derivative()\n",
        "      # calculate dloss\n",
        "      dloss += 2 * mse * dcirc\n",
        "\n",
        "    return dloss, loss/len(self.data)\n",
        "  \n",
        "\n",
        "  def gradient_descent(self, learning_rate, epochs):\n",
        "    \"\"\"This function performs a full gradient descent strategy.\"\"\"\n",
        "\n",
        "    # we create a folder\n",
        "    os.system(\"mkdir -p ./live-plotting\")\n",
        "    # we clean it if already exists\n",
        "    os.system(\"rm ./live-plotting/*.png\")\n",
        "\n",
        "    # we want to keep track of the loss function\n",
        "    loss_history = []\n",
        "\n",
        "    # the gradient descent strategy\n",
        "    for epoch in range(epochs):\n",
        "      dloss, loss = self.evaluate_loss_gradients()\n",
        "      loss_history.append(loss)\n",
        "      self.params -= learning_rate * dloss\n",
        "      print(f'Loss at epoch: {epoch + 1} ', loss)\n",
        "\n",
        "      self.show_predictions(f'Epoch {epoch +1}', save=True)\n",
        "    \n",
        "    return loss_history\n",
        "\n",
        "\n",
        "# ---------------------- PLOTTING FUNCTION -------------------------------------\n",
        "\n",
        "  def show_predictions(self, title, save=False):\n",
        "    \"\"\"This function shows the obtained results through a scatter plot.\"\"\"\n",
        "\n",
        "    # calculate prediction\n",
        "    predictions = self.predict_sample()\n",
        "\n",
        "    # draw the results\n",
        "    plt.figure(figsize=(12,8))\n",
        "    plt.title(title)\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('y')\n",
        "    plt.scatter(self.data, self.labels, color='orange', alpha=0.6, label='Original', s=70, marker='o')\n",
        "    plt.scatter(self.data, predictions, color='purple', alpha=0.6, label='Predictions', s=70, marker='o')\n",
        "\n",
        "    plt.legend()\n",
        "\n",
        "    # we save all the images during the training in order to see the evolution\n",
        "    if save:\n",
        "      plt.savefig(f'./live-plotting/'+str(title)+'.png')\n",
        "      plt.close()\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "K9Ev6sVK4RC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate a sample\n",
        "\n",
        "We are going to fit a very simple function:\n",
        "\n",
        "$$ y = \\sin(2x).$$"
      ],
      "metadata": {
        "id": "4BzlNQ_gRr4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ndata = 30\n",
        "# random data\n",
        "data = np.random.uniform(-1, 1, ndata)\n",
        "# labeling them\n",
        "labels = np.sin(2*data)"
      ],
      "metadata": {
        "id": "I4xKVEur03Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the QML algorithm\n",
        "VQR = vqregressor(layers=1, data=data, labels=labels)"
      ],
      "metadata": {
        "id": "3m7F5YKb0qxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show initial (WRONG) predictions\n",
        "VQR.show_predictions('Without training')"
      ],
      "metadata": {
        "id": "VsBxxIby4uMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set the training hyper-parameters\n",
        "epochs = 50\n",
        "learning_rate = 0.025\n",
        "\n",
        "# perform the training\n",
        "history = VQR.gradient_descent(learning_rate=learning_rate, epochs=epochs)"
      ],
      "metadata": {
        "id": "S9L-RCc4hZ0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# showing loss history\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.title('Loss history')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss value')\n",
        "plt.plot(history, lw=3, c='purple', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BIZYUD--TD0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final results\n",
        "VQR.show_predictions('After training')"
      ],
      "metadata": {
        "id": "1GHSr9YSAuA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's visualize the training with a gif"
      ],
      "metadata": {
        "id": "YuQOnLLhT7Z5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "images = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  images.append(Image.open(\"./live-plotting/Epoch \" + str(epoch + 1) + \".png\"))\n",
        "\n",
        "first_image = images[0]\n",
        "first_image.save(\"./training.gif\", format=\"GIF\", append_images=images,\n",
        "               save_all=True, duration=100, loop=0)"
      ],
      "metadata": {
        "id": "9hWH63k5610I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A real gradient descent on hardware\n",
        "\n",
        "<center><img src=\"https://github.com/MatteoRobbiati/notebooks/blob/main/QTI-QML-tutorial/images/on-hdw.png?raw=true\" alt=\"drawing\" width=\"800\"/></center>\n",
        "\n",
        "Reference: [arXiv:2210.10787](https://arxiv.org/abs/2210.10787)."
      ],
      "metadata": {
        "id": "19Rl8Uo2hQQO"
      }
    }
  ]
}